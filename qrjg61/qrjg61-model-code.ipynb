{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation would not be possible without the article provided by Hugging face and paper written by Ho et al., 2017 as I referenced them quite often in order to try implement and fix bugs located in the code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter code block\n",
    "image_dimensions = 32 # get 32 x 32, 64 x 64 or 128 x 128 images\n",
    "batch_size_param = 32 # We will start with batch size 32 and iteratively reduce depending what the limit of NCC and Google colab is\n",
    "n_channels = 3 # RGB image has 3 channels and greyscale has 1\n",
    "dataset_list = ['stl10', 'cifar10', 'ffhq']\n",
    "dataset = dataset_list[0] # for now we will attempt to use the stl10 dataset however we may use the cifar10 dataset if this does not work or upgrade to the ffhq if we have the time\n",
    "dd = None\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # If GPU is available we will use that otherwise cpu (realistically we can only use a gpu as cpu will train too long)\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataloader implementation based on code provided for the assingment by Amir and Chris\"\"\"\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "data_transforms = [transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "#                    transforms.Grayscale(num_output_channels=1), -> if we want to use grayscale images on higher dimensionality data and the training is taking too long we will enable this option\n",
    "                   transforms.RandomHorizontalFlip(),\n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Lambda(lambda t: (t * 2) - 1)]\n",
    "\n",
    "data_transform = transforms.Compose(data_transforms)\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    train = torchvision.datasets.CIFAR10('drive/My Drive/training/cifar10', download=True, transform=data_transform)\n",
    "\n",
    "    test = torchvision.datasets.CIFAR10('drive/My Drive/training/cifar10', train=False, download=True, transform=data_transform)\n",
    "    transformed_dataset = torch.utils.data.ConcatDataset([train, test])\n",
    "\n",
    "\n",
    "# stl10 has larger images which are much slower to train on. You should develop your method with CIFAR-10 before experimenting with STL-10\n",
    "if dataset == 'stl10':\n",
    "    train = torchvision.datasets.STL10('drive/My Drive/training/stl10', transform=data_transform, split='train')\n",
    "    test = torchvision.datasets.STL10('drive/My Drive/training/stl10', transform=data_transform, split='test')\n",
    "    transformed_dataset = torch.utils.data.ConcatDataset([train, test])\n",
    "\n",
    "if dataset == \"ffhq\":\n",
    "\n",
    "    # Set batch size\n",
    "\n",
    "    IMG_SIZE = 128\n",
    "    BATCH_SIZE = 16\n",
    "\n",
    "    #         transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    #         transforms.RandomHorizontalFlip(),\n",
    "    #         transforms.ToTensor(), # Scales data into [0,1] \n",
    "    #         transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1] \n",
    "    #     ]\n",
    "\n",
    "\n",
    "    # Define the transform to resize the images to 128x128 and convert them to tensors\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "        ])\n",
    "\n",
    "    # Load the dataset using the ImageFolder dataset\n",
    "    dataset = datasets.ImageFolder(root=\"./images/thumbnails128x128\", transform=transform)\n",
    "\n",
    "    # Create a DataLoader for the dataset with the specified batch size\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Create empty lists for the images and labels\n",
    "    images_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Loop over the batches and append the images and labels to the lists\n",
    "    for images, labels in dataloader:\n",
    "        images_list.append(images)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    # Concatenate the lists of images and labels into PyTorch tensors\n",
    "    images_tensor = torch.cat(images_list, dim=0)\n",
    "    labels_tensor = torch.cat(labels_list, dim=0)\n",
    "\n",
    "    # Create a TensorDataset with the concatenated tensors\n",
    "    data= TensorDataset(images_tensor, labels_tensor)\n",
    "    dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Below mathematical python implementation based on DDPM paper and help by referencing huggingface implentation to understand how to perform certain calculation in python e.g. cumulative production\"\"\"\n",
    "\n",
    "def forward_diffusion(x_0, t, device='cpu'):\n",
    "    \n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_cumulative_alphas_t = get_index(cumulative_sqrt_alphas, t, x_0.shape)\n",
    "    sqrt_one_minus_cumulative_alphas_t = get_index(sqrt_one_minus_cumulative_alphas, t, x_0.shape)\n",
    "    \n",
    "    return sqrt_cumulative_alphas_t.to(device) * x_0.to(device) \\\n",
    "    + sqrt_one_minus_cumulative_alphas_t.to(device) * noise.to(device), noise.to(device)\n",
    "    \n",
    "\n",
    "\n",
    "def get_index(vals, t, x_shape):\n",
    "    \"\"\" \n",
    "    Returns a specific index t of a passed list of values vals\n",
    "    while considering the batch dimension.\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "\n",
    "def cosine_beta_schedule(t, s=0.00001):\n",
    "    \"\"\"\n",
    "    cosine beta scheduler obtained from: https://huggingface.co/blog/annotated-diffusion\n",
    "    Initially proposed by: https://arxiv.org/abs/2102.09672\n",
    "    \n",
    "    \"\"\"\n",
    "    n_steps = t + 1\n",
    "    x = torch.linspace(0, t, n_steps) # Creates a tesnor of evenly spaced values from start to end\n",
    "    t_over_T_plus_s = x/t + s # t/T + s\n",
    "    one_plus_s = 1 + s # 1 + s\n",
    "    pi_over_2 = torch.pi * 0.5\n",
    "    \n",
    "    f_t = torch.cos((t_over_T_plus_s/one_plus_s) * pi_over_2)**2\n",
    "    cumulative_product = f_t/f_t[0]\n",
    "    betas = 1 - (cumulative_product[1:])/cumulative_product[:-1] # which is equal (cumulative_product)/cumulative_product[:-2]\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "# Define beta schedule\n",
    "T = 100\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1. - betas\n",
    "cumulative_alphas = torch.cumprod(alphas, axis=0)\n",
    "cumulative_alphas_prev = F.pad(cumulative_alphas[:-1], (1, 0), value=1.0)\n",
    "sqrt_one_over_alphas = torch.sqrt(1.0 / alphas)\n",
    "cumulative_sqrt_alphas = torch.sqrt(cumulative_alphas)\n",
    "sqrt_one_minus_cumulative_alphas = torch.sqrt(1. - cumulative_alphas)\n",
    "posterior_variance = betas * (1. - cumulative_alphas_prev) / (1. - cumulative_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def show_tensor_image(image):\n",
    "    \n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "        transforms.Lambda(lambda t: t * 255.), \n",
    "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "    \n",
    "    if len(image.shape) == 4:\n",
    "        \n",
    "        image = image[0, :, :, :]\n",
    "        \n",
    "    plt.imshow(reverse_transforms(image))\n",
    "\n",
    "\n",
    "if dd is None:\n",
    "    data = transformed_dataset\n",
    "    dataloader = DataLoader(data, batch_size =BATCH_SIZE, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate forward diffusion\n",
    "image = next(iter(dataloader))[0]\n",
    "\n",
    "plt.figure(figsize=(94,94))\n",
    "plt.axis('off')\n",
    "num_images = 10\n",
    "stepsize = int(T/num_images)\n",
    "\n",
    "for idx in range(0, T, stepsize):\n",
    "    t = torch.Tensor([idx]).type(torch.int64)\n",
    "    plt.subplot(1, num_images+1, int((idx/stepsize) + 1))\n",
    "    image, noise = forward_diffusion(image, t)\n",
    "    show_tensor_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The below code is adapted from the code provided by huggingface @ https://huggingface.co/blog/annotated-diffusion\"\"\"\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        \n",
    "        self.q_conv = nn.Conv2d(in_channels, in_channels , kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Compute query, key, and value\n",
    "        proj_query = self.q_conv(x)\n",
    "        proj_key = proj_query\n",
    "        proj_value = x\n",
    "        \n",
    "        # Compute attention map\n",
    "        energy = torch.matmul(proj_query, proj_key.transpose(2, 3))\n",
    "        attention = self.softmax(energy)\n",
    "        \n",
    "        # Compute attended feature maps\n",
    "        out = torch.matmul(attention, proj_value)\n",
    "        out = out + x\n",
    "        \n",
    "        # Apply gamma scaling\n",
    "        out = self.gamma * out\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "\n",
    "class SampleBlock(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, in_ch, out_ch, time_embedding_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_embedding =  nn.Linear(time_embedding_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU()\n",
    "        \n",
    "        self.attention = AttentionBlock(out_ch)\n",
    "        \n",
    "    def forward(self, x, t, ):\n",
    "        \n",
    "        # First Convolution\n",
    "        h = self.conv1(x)\n",
    "        h = self.relu(h)\n",
    "        h = self.bnorm1(h)\n",
    "        \n",
    "        # Create time embedding\n",
    "        time_embedding = self.time_embedding(t)\n",
    "        time_embedding = self.relu(time_embedding)\n",
    "        \n",
    "        time_embedding = time_embedding[(..., ) + (None, ) * 2]\n",
    "        \n",
    "        # Add time embedding to tensor\n",
    "        h = h + time_embedding\n",
    "        \n",
    "        # Second Convolution\n",
    "        h = self.conv2(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.bnorm2(h)\n",
    "        \n",
    "        # Apply attention mechanisms\n",
    "        h = self.attention(h)\n",
    "        \n",
    "        \n",
    "        # Downsample or Upsample\n",
    "        \n",
    "        return self.transform(h)\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "\n",
    "        return embeddings\n",
    "        \n",
    "class U_Net(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        image_channels = 3\n",
    "        down_channels = (64, 128, 256, 512, 1024)\n",
    "        up_channels = (1024, 512, 256, 128, 64)\n",
    "        out_dim = 1\n",
    "        time_embedding_dim = 64\n",
    "\n",
    "\n",
    "        self.time_embedding = nn.Sequential(\n",
    "        SinusoidalPositionEmbeddings(time_embedding_dim),\n",
    "        nn.Linear(time_embedding_dim, time_embedding_dim),\n",
    "        nn.ReLU())\n",
    "\n",
    "        # Input convolution to project image to initial image channels\n",
    "        self.input_conv = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        self.downs = nn.ModuleList([SampleBlock(down_channels[i], down_channels[i+1], time_embedding_dim)\\\n",
    "                                   for i in range(len(down_channels) - 1)])\n",
    "\n",
    "        self.ups = nn.ModuleList([SampleBlock(up_channels[i], up_channels[i+1], time_embedding_dim, up=True)\\\n",
    "                                   for i in range(len(down_channels) - 1)])\n",
    "\n",
    "        self.output_conv = nn.Conv2d(up_channels[-1], 3, out_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "\n",
    "        t = self.time_embedding(timestep)\n",
    "\n",
    "        x = self.input_conv(x)\n",
    "\n",
    "        residual_inputs = []\n",
    "\n",
    "        for down in self.downs:\n",
    "\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "\n",
    "        for up in self.ups:\n",
    "\n",
    "            residual_x = residual_inputs.pop()\n",
    "\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t)\n",
    "\n",
    "        return self.output_conv(x)\n",
    "\n",
    "        \n",
    "model = U_Net()\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Due to the instability of the NCC I would save the model weights of my model at the end of every training itteration and \n",
    "load them in for the next training session. This was done for the purpose of making sure I could train the model for an ample amount\n",
    "of time\"\"\"\n",
    "\n",
    "model.load_state_dict(torch.load(\"model_weights_log.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "\n",
    "def negative_log_likelihood_loss(x_true, x_pred):\n",
    "    \"\"\"\n",
    "    Compute the negative log-likelihood loss function for a diffusion model.\n",
    "\n",
    "    Arguments:\n",
    "        x_true: Tensor of shape (batch_size, channels, height, width) containing the true images.\n",
    "        x_pred: Tensor of shape (batch_size, channels, height, width) containing the predicted images.\n",
    "\n",
    "    Returns:\n",
    "        loss: Scalar Tensor containing the negative log-likelihood loss.\n",
    "    \"\"\"\n",
    "    batch_size, channels, height, width = x_true.shape\n",
    "\n",
    "    # Reshape input images to (batch_size, channels * height * width)\n",
    "    x_true_flat = x_true.reshape(batch_size, -1)\n",
    "    x_pred_flat = x_pred.reshape(batch_size, -1)\n",
    "\n",
    "    # Compute the negative log-likelihood of the true images given a normal distribution with\n",
    "    # mean equal to the predicted images and standard deviation of 1.0\n",
    "    likelihood = tdist.Normal(loc=x_pred_flat, scale=1.0)\n",
    "    log_probs = likelihood.log_prob(x_true_flat)\n",
    "    loss = -torch.mean(log_probs)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, x_0, t):\n",
    "    x_noisy, noise = forward_diffusion(x_0, t, device)\n",
    "    noise_pred = model(x_noisy, t)\n",
    "#     return F.l1_loss(noise, noise_pred)\n",
    "    return negative_log_likelihood_loss(noise,noise_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The below functions are based on adapting the implementations provided by Hugging face @ https://huggingface.co/blog/annotated-diffusion \"\"\"\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_timestep(x, t):\n",
    "    \"\"\"\n",
    "    Calls the model to predict the noise in the image and returns \n",
    "    the denoised image. \n",
    "    Applies noise to this image, if we are not in the last step yet.\n",
    "    \"\"\"\n",
    "    betas_t = get_index(betas, t, x.shape)\n",
    "    sqrt_one_minus_cumulative_alphas_t = get_index(\n",
    "        sqrt_one_minus_cumulative_alphas, t, x.shape\n",
    "    )\n",
    "    sqrt_one_over_alphas_t = get_index(sqrt_one_over_alphas, t, x.shape)\n",
    "    \n",
    "    # Call model (current image - noise prediction)\n",
    "    model_mean = sqrt_one_over_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_cumulative_alphas_t\n",
    "    )\n",
    "    posterior_variance_t = get_index(posterior_variance, t, x.shape)\n",
    "    \n",
    "    if t == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        noise = torch.randn_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_plot_image():\n",
    "    # Sample noise\n",
    "    img_size = IMG_SIZE\n",
    "    img = torch.randn((1, 3, img_size, img_size), device=device)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.axis('off')\n",
    "    num_images = 10\n",
    "    stepsize = int(T/num_images)\n",
    "\n",
    "    for i in range(0,T)[::-1]:\n",
    "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "        img = sample_timestep(img, t)\n",
    "        if i % stepsize == 0:\n",
    "            plt.subplot(1, num_images, int(i/stepsize+1))\n",
    "            show_tensor_image(img.detach().cpu())\n",
    "    plt.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "losses = []\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # batch[0] (len 128) -> image tensors batch[1] (len 128)\n",
    "        clean_images = batch[0].to(device)\n",
    "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "        bs = clean_images.shape[0]\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=clean_images.device).long()\n",
    "\n",
    "        noisy_images, noise = forward_diffusion(batch[0], t, device)\n",
    "\n",
    "        noise_pred = model(noisy_images, t)\n",
    "\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        loss.backward(loss)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if ((epoch + 1) % 5 == 0) and (step == 0):\n",
    "            loss_last_epoch = sum(losses[-len(dataloader) :]) / len(dataloader)\n",
    "            sample_plot_image()\n",
    "            print(f\"Epoch:{epoch+1}, loss: {loss_last_epoch}\")\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_plot_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started with MAE then tried MSE and both those yielded ok but not great results. I have moved on to try use log likelihood as an error function hopefully this will improve my model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from torch.optim import Adam\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "# optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "# epochs = 100 # Try more!\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     for step, batch in enumerate(dataloader):\n",
    "#         optimizer.zero_grad()\n",
    "#         clean_images = batch\n",
    "#         t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
    "#         loss = get_loss(model, batch[0], t)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if (step == 0) and (epochs % 10 == 0):\n",
    "#             print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "#             sample_plot_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_plot_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim import Adam\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "# optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "# epochs = 100 # Try more!\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     for step, batch in enumerate(dataloader):\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
    "#         loss = get_loss(model, batch[0], t)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if (step == 0) and (epochs % 10 == 0):\n",
    "#             print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "#             sample_plot_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_weights_log.pth\")\n",
    "# net.load_state_dict(torch.load(\"model_weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpm",
   "language": "python",
   "name": "ddpm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
